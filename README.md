# Contenedor Ollama con WebUI en Docker Compose

Este proyecto te permite ejecutar m√∫ltiples modelos de IA usando [Ollama](https://ollama.com) en contenedores Docker, incluyendo [OpenCoder](https://github.com/OpenCoder-llm/OpenCoder-llm), [DeepSeek](https://ollama.com/library/deepseek) y [GenAI Image CSPrompt](https://ollama.com/ALIENTELLIGENCE/genaiimagecsprompt), con una interfaz web moderna para gestionar todos los modelos.

## üöÄ Caracter√≠sticas Principales

- **Arquitectura simplificada**: Un solo servicio Ollama que maneja m√∫ltiples modelos
- **Interfaz web moderna**: [Open WebUI](https://github.com/open-webui/open-webui) para gesti√≥n visual
- **M√∫ltiples modelos**: Soporte para 3 familias de modelos diferentes

## ü§ñ Modelos Disponibles

### OpenCoder
- **Descripci√≥n**: Familia de modelos de lenguaje de c√≥digo abierto y reproducible
- **Modelos**: OpenCoder-1.5B (1.4GB) y OpenCoder-8B (4.7GB)
- **Ventana de contexto**: 4K para 1.5B, 8K para 8B
- **Soporte multiling√ºe**: Ingl√©s y chino
- **Especialidad**: Generaci√≥n de c√≥digo, programaci√≥n general

### DeepSeek
- **DeepSeek Coder**: Modelo especializado en programaci√≥n y algoritmos complejos
  - **6.7B**: ~6.7GB, contexto 32K
  - **1.3B**: Modelo ligero para tareas b√°sicas
- **DeepSeek-R1**: Modelo de razonamiento avanzado
  - **1.5B**: 1.1GB, modelo ligero
  - **7B**: 4.7GB, equilibrio rendimiento/tama√±o
  - **8B**: 5.2GB, rendimiento mejorado
  - **14B**: 9.0GB, alta precisi√≥n
  - **32B**: 20GB, m√°xima precisi√≥n
  - **70B**: 43GB, estado del arte

### GenAI Image CSPrompt
- **Descripci√≥n**: Generador de prompts para im√°genes de IA
- **Tama√±o**: 4.7GB
- **Ventana de contexto**: 8K
- **Especialidad**: Crear prompts detallados para DALL-E, Midjourney, Stable Diffusion

## üìã Requisitos

- Docker
- Docker Compose
- Al menos 8GB de RAM libre (recomendado 16GB+)
- 20GB de espacio en disco
- Python 3.7+ (para scripts de prueba)

## üõ†Ô∏è Instalaci√≥n y Uso

### 1. Inicio r√°pido

```bash
# Iniciar servicios b√°sicos
./ollama.sh start

# O iniciar con interfaz web
./ollama.sh start webui
```

### 2. Gesti√≥n con el script principal

```bash
# Iniciar servicios
./ollama.sh start              # Solo Ollama
./ollama.sh start webui        # Ollama + interfaz web

# Descargar modelos
./ollama.sh pull opencoder:1.5b
./ollama.sh pull deepseek-coder:6.7b
./ollama.sh pull deepseek-r1:1.5b
./ollama.sh pull alientelligence/genaiimagecsprompt

# Ver logs
./ollama.sh logs ollama
./ollama.sh logs webui

# Probar API
./ollama.sh test

# Ver estado
./ollama.sh status

# Listar modelos
./ollama.sh list

# Detener servicios
./ollama.sh stop
```

### 3. Tests autom√°ticos

```bash
# Probar todos los modelos disponibles
python3 test/test_all_models.py

# Probar modelo espec√≠fico
python3 test/test_ollama.py
```

## üåê Interfaz Web

### Open WebUI
- **URL**: http://localhost:8080
- **Caracter√≠sticas**:
  - Gesti√≥n visual de modelos
  - Chat interactivo con m√∫ltiples modelos
  - Configuraci√≥n de par√°metros
  - Historial de conversaciones
  - Gesti√≥n de usuarios

### Configuraci√≥n de variables de entorno

Crea un archivo `.env` en la ra√≠z del proyecto:

```env
OLLAMA_BASE_URL=http://ollama:11434
WEBUI_SECRET_KEY=tu_clave_secreta_aqui
DEFAULT_USER_ROLE=admin
ENABLE_SIGNUP=true
ENABLE_LOGIN_FORM=true
```

## üîå Puertos y Servicios

- **Ollama API**: `http://localhost:11434`
- **Open WebUI**: `http://localhost:8080`

## üìù Ejemplos de Uso

### API REST - OpenCoder

```bash
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "opencoder:8b",
    "prompt": "Escribe una funci√≥n de ordenamiento r√°pido en Python",
    "stream": false
  }'
```

### API REST - DeepSeek

```bash
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "deepseek-coder:6.7b",
    "prompt": "Implementa un algoritmo de machine learning completo con visualizaci√≥n",
    "stream": false
  }'
```

### API REST - GenAI Image CSPrompt

```bash
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "alientelligence/genaiimagecsprompt",
    "prompt": "Crea un prompt para generar una imagen de un paisaje de monta√±a al atardecer",
    "stream": false
  }'
```

## üß™ Scripts de Prueba

### Test Autom√°tico de Modelos

```bash
python3 test/test_all_models.py
```

Este script:
- Detecta autom√°ticamente todos los modelos disponibles
- Ejecuta prompts espec√≠ficos para cada tipo de modelo
- Muestra m√©tricas de rendimiento (tiempo, tokens)
- Genera un reporte completo de resultados

### Test Individual

```bash
python3 test/test_ollama.py
```

## üìä Gesti√≥n de Modelos

### Listar modelos disponibles
```bash
curl http://localhost:11434/api/tags
```

### Eliminar un modelo
```bash
curl -X DELETE http://localhost:11434/api/delete \
  -H "Content-Type: application/json" \
  -d '{"name": "opencoder:8b"}'
```

### Informaci√≥n del modelo
```bash
curl http://localhost:11434/api/show \
  -H "Content-Type: application/json" \
  -d '{"name": "opencoder:8b"}'
```

## ‚öôÔ∏è Configuraci√≥n Avanzada

### Variables de entorno

Puedes modificar las variables de entorno en el `docker-compose.yml`:

- `OLLAMA_HOST`: Host de escucha (por defecto: 0.0.0.0)
- `OLLAMA_ORIGINS`: Or√≠genes permitidos para CORS

### Vol√∫menes

Los datos se almacenan en bind mounts para f√°cil acceso:

- `./data/ollama`: Modelos y configuraci√≥n de Ollama
- `./data/open-webui`: Datos de la interfaz web

### Recursos del sistema

Para optimizar el rendimiento, puedes agregar l√≠mites de recursos:

```yaml
services:
  ollama:
    # ... configuraci√≥n existente ...
    deploy:
      resources:
        limits:
          memory: 16G
          cpus: '8.0'
        reservations:
          memory: 8G
          cpus: '4.0'
```

## üîß Soluci√≥n de Problemas

### El contenedor no inicia

1. Verifica que tienes suficiente espacio en disco
2. Aseg√∫rate de que los puertos no est√©n en uso
3. Revisa los logs: `./ollama.sh logs ollama`

### Error de memoria

Si obtienes errores de memoria insuficiente:

1. Reduce el n√∫mero de modelos ejecut√°ndose simult√°neamente
2. Usa solo modelos m√°s peque√±os (OpenCoder 1.5B, DeepSeek-R1 1.5B)
3. Aumenta la memoria disponible para Docker

### Modelo no responde

1. Verifica que el modelo se descarg√≥ correctamente: `./ollama.sh list`
2. Revisa los logs del contenedor: `./ollama.sh logs ollama`
3. Intenta reiniciar el servicio: `./ollama.sh restart`

### Problemas con la interfaz web

1. Verifica que las variables de entorno est√°n configuradas correctamente
2. Revisa los logs: `./ollama.sh logs webui`
3. Aseg√∫rate de que Ollama est√° ejecut√°ndose antes de iniciar la web UI

## üìà Comparaci√≥n de Modelos

| Modelo | Tama√±o | Contexto | Especialidad | Uso Recomendado |
|--------|--------|----------|--------------|-----------------|
| OpenCoder 1.5B | 1.4GB | 4K | C√≥digo general | Desarrollo r√°pido |
| OpenCoder 8B | 4.7GB | 8K | C√≥digo avanzado | Proyectos complejos |
| DeepSeek Coder 1.3B | ~1GB | 4K | Programaci√≥n b√°sica | Aprendizaje |
| DeepSeek Coder 6.7B | 6.7GB | 32K | Algoritmos complejos | ML, optimizaci√≥n |
| DeepSeek-R1 1.5B | 1.1GB | 4K | Razonamiento b√°sico | L√≥gica simple |
| DeepSeek-R1 8B | 5.2GB | 8K | Razonamiento avanzado | Problemas complejos |
| DeepSeek-R1 32B | 20GB | 32K | M√°xima precisi√≥n | Investigaci√≥n |
| GenAI Image CSPrompt | 4.7GB | 8K | Prompts para im√°genes | Generaci√≥n de contenido |

## üìö Referencias

- [Repositorio de OpenCoder](https://github.com/OpenCoder-llm/OpenCoder-llm)
- [DeepSeek en Ollama](https://ollama.com/library/deepseek)
- [GenAI Image CSPrompt en Ollama](https://ollama.com/ALIENTELLIGENCE/genaiimagecsprompt)
- [Open WebUI](https://github.com/open-webui/open-webui)
- [Documentaci√≥n de Ollama](https://ollama.com/docs)
- [Paper de OpenCoder](https://arxiv.org/pdf/2411.04905)

## üìÑ Licencia

Este proyecto est√° bajo la licencia MIT, al igual que los modelos incluidos.


