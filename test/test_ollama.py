#!/usr/bin/env python3
"""
Script unificado para probar modelos de Ollama
Uso: python test_ollama.py [modelo] [prompt]
"""

import requests
import json
import sys
import time
from typing import Dict, Any, Optional

# Configuraci√≥n
OLLAMA_BASE_URL = "http://localhost:11434"
DEFAULT_MODEL = "deepseek-r1:1.5b"  # Modelo por defecto

# Modelos disponibles para testing
AVAILABLE_MODELS = {
    "opencoder-8b": "opencoder:8b",
    "opencoder-1.5b": "opencoder:1.5b", 
    "deepseek-coder-6.7b": "deepseek-coder:6.7b",
    "deepseek-coder-1.3b": "deepseek-coder:1.3b",
    "deepseek-r1": "deepseek-r1",
    "deepseek-r1-1.5b": "deepseek-r1:1.5b",
    "deepseek-r1-7b": "deepseek-r1:7b",
    "deepseek-r1-8b": "deepseek-r1:8b",
    "deepseek-r1-14b": "deepseek-r1:14b",
    "deepseek-r1-32b": "deepseek-r1:32b",
    "deepseek-r1-70b": "deepseek-r1:70b",
    "genaiimagecsprompt": "alientelligence/genaiimagecsprompt"
}

# Prompts de ejemplo para diferentes tipos de modelos
EXAMPLE_PROMPTS = {
    "coding": "Escribe una funci√≥n en Python que calcule el factorial de un n√∫mero usando recursi√≥n.",
    "reasoning": "Si tengo 3 manzanas y me como 1, luego compro 2 m√°s, ¬øcu√°ntas manzanas tengo? Explica tu razonamiento paso a paso.",
    "image_prompt": "Genera un prompt detallado para crear una imagen de un gato espacial viajando en una nave espacial futurista.",
    "general": "Explica brevemente qu√© es la inteligencia artificial y sus aplicaciones principales."
}

def check_ollama_status() -> bool:
    """Verificar si Ollama est√° ejecut√°ndose"""
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/version", timeout=5)
        if response.status_code == 200:
            version = response.json().get("version", "unknown")
            print(f"‚úÖ Ollama est√° ejecut√°ndose (versi√≥n: {version})")
            return True
        else:
            print(f"‚ùå Error al conectar con Ollama: {response.status_code}")
            return False
    except requests.exceptions.RequestException as e:
        print(f"‚ùå No se puede conectar con Ollama: {e}")
        print(f"   Aseg√∫rate de que Ollama est√© ejecut√°ndose en {OLLAMA_BASE_URL}")
        return False

def list_available_models() -> list:
    """Listar modelos disponibles en Ollama"""
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=10)
        if response.status_code == 200:
            models = response.json().get("models", [])
            if models:
                print("üìã Modelos disponibles:")
                for model in models:
                    name = model.get("name", "unknown")
                    size = model.get("size", 0)
                    size_gb = size / (1024**3) if size > 0 else 0
                    print(f"   ‚Ä¢ {name} ({size_gb:.1f}GB)")
                return [model.get("name") for model in models]
            else:
                print("üìã No hay modelos descargados")
                return []
        else:
            print(f"‚ùå Error al obtener modelos: {response.status_code}")
            return []
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Error al conectar con Ollama: {e}")
        return []

def test_model(model_name: str, prompt: str, max_tokens: int = 500) -> bool:
    """Probar un modelo espec√≠fico con un prompt"""
    print(f"\nü§ñ Probando modelo: {model_name}")
    print(f"üìù Prompt: {prompt[:100]}{'...' if len(prompt) > 100 else ''}")
    print("-" * 60)
    
    # Preparar la solicitud
    payload = {
        "model": model_name,
        "prompt": prompt,
        "stream": False,
        "options": {
            "num_predict": max_tokens,
            "temperature": 0.7,
            "top_p": 0.9
        }
    }
    
    try:
        start_time = time.time()
        response = requests.post(
            f"{OLLAMA_BASE_URL}/api/generate",
            json=payload,
            timeout=120  # 2 minutos de timeout
        )
        end_time = time.time()
        
        if response.status_code == 200:
            result = response.json()
            response_text = result.get("response", "")
            total_duration = result.get("total_duration", 0) / 1e9  # Convertir a segundos
            
            print(f"‚úÖ Respuesta ({total_duration:.2f}s):")
            print(response_text)
            print(f"\n‚è±Ô∏è  Tiempo total: {total_duration:.2f}s")
            print(f"üìä Tokens generados: {result.get('eval_count', 0)}")
            return True
        else:
            error_msg = response.text
            print(f"‚ùå Error en la respuesta: {response.status_code}")
            print(f"   Detalles: {error_msg}")
            return False
            
    except requests.exceptions.Timeout:
        print("‚ùå Timeout: La respuesta tard√≥ demasiado")
        return False
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Error de conexi√≥n: {e}")
        return False

def test_image_generation(model_name: str, prompt: str) -> bool:
    """Probar generaci√≥n de im√°genes (para modelos como SDXL)"""
    print(f"\nüé® Probando generaci√≥n de imagen con: {model_name}")
    print(f"üìù Prompt: {prompt}")
    print("-" * 60)
    
    payload = {
        "model": model_name,
        "prompt": prompt,
        "stream": False
    }
    
    try:
        start_time = time.time()
        response = requests.post(
            f"{OLLAMA_BASE_URL}/api/generate",
            json=payload,
            timeout=300  # 5 minutos para generaci√≥n de im√°genes
        )
        end_time = time.time()
        
        if response.status_code == 200:
            result = response.json()
            print("‚úÖ Imagen generada exitosamente")
            print(f"‚è±Ô∏è  Tiempo: {end_time - start_time:.2f}s")
            return True
        else:
            print(f"‚ùå Error: {response.status_code} - {response.text}")
            return False
            
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Error: {e}")
        return False

def show_help():
    """Mostrar ayuda del script"""
    print("""
ü§ñ Test Unificado de Ollama
==========================

Uso: python test_ollama.py [modelo] [prompt]

Argumentos:
  modelo    Nombre del modelo a probar (opcional)
  prompt    Prompt personalizado (opcional)

Modelos disponibles:
  opencoder-8b          - OpenCoder 8B (c√≥digo)
  opencoder-1.5b        - OpenCoder 1.5B (c√≥digo ligero)
  deepseek-coder-6.7b   - DeepSeek Coder 6.7B
  deepseek-coder-1.3b   - DeepSeek Coder 1.3B
  deepseek-r1           - DeepSeek-R1 (razonamiento)
  deepseek-r1-1.5b      - DeepSeek-R1 1.5B (razonamiento ligero)
  deepseek-r1-7b        - DeepSeek-R1 7B
  deepseek-r1-8b        - DeepSeek-R1 8B
  deepseek-r1-14b       - DeepSeek-R1 14B
  deepseek-r1-32b       - DeepSeek-R1 32B
  deepseek-r1-70b       - DeepSeek-R1 70B
  genaiimagecsprompt    - Generador de prompts para im√°genes

Ejemplos:
  python test_ollama.py
  python test_ollama.py deepseek-r1-1.5b
  python test_ollama.py opencoder-1.5b "Escribe una funci√≥n de Fibonacci"
  python test_ollama.py genaiimagecsprompt "gato espacial"
""")

def main():
    """Funci√≥n principal"""
    print("ü§ñ Test Unificado de Ollama")
    print("=" * 40)
    
    # Verificar argumentos
    if len(sys.argv) > 1 and sys.argv[1] in ["-h", "--help", "help"]:
        show_help()
        return
    
    # Verificar estado de Ollama
    if not check_ollama_status():
        return
    
    # Listar modelos disponibles
    available_models = list_available_models()
    
    # Determinar modelo a usar
    if len(sys.argv) > 1:
        model_key = sys.argv[1]
        if model_key in AVAILABLE_MODELS:
            model_name = AVAILABLE_MODELS[model_key]
        else:
            print(f"‚ùå Modelo '{model_key}' no reconocido")
            print("   Usa 'python test_ollama.py help' para ver modelos disponibles")
            return
    else:
        model_name = DEFAULT_MODEL
        print(f"üìã Usando modelo por defecto: {model_name}")
    
    # Verificar si el modelo est√° disponible
    if model_name not in available_models:
        print(f"‚ö†Ô∏è  Modelo '{model_name}' no est√° descargado")
        print(f"   Desc√°rgalo con: ./ollama.sh pull {model_name}")
        return
    
    # Determinar prompt a usar
    if len(sys.argv) > 2:
        prompt = " ".join(sys.argv[2:])
    else:
        # Seleccionar prompt basado en el tipo de modelo
        if "coder" in model_name or "opencoder" in model_name:
            prompt = EXAMPLE_PROMPTS["coding"]
        elif "r1" in model_name:
            prompt = EXAMPLE_PROMPTS["reasoning"]
        elif "genaiimagecsprompt" in model_name:
            prompt = EXAMPLE_PROMPTS["image_prompt"]
        else:
            prompt = EXAMPLE_PROMPTS["general"]
    
    # Ejecutar test
    if "genaiimagecsprompt" in model_name:
        success = test_image_generation(model_name, prompt)
    else:
        success = test_model(model_name, prompt)
    
    if success:
        print("\n‚úÖ Test completado exitosamente!")
    else:
        print("\n‚ùå Test fall√≥")
        sys.exit(1)

if __name__ == "__main__":
    main() 